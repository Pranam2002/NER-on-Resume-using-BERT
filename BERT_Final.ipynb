{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installing necessary packages and libraries"
      ],
      "metadata": {
        "id": "fU9XnK8XToZ0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "0R9bHXxkR3vy"
      },
      "outputs": [],
      "source": [
        "# Install seqeval library version 0.0.12 for sequence labeling evaluation\n",
        "!pip install -q seqeval==0.0.12\n",
        "\n",
        "# Install transformers library for working with pre-trained language models, including BERT\n",
        "!pip install -q transformers\n",
        "\n",
        "# Install pytorch-pretrained-bert library for PyTorch implementation of pre-trained BERT models\n",
        "!pip install -q pytorch-pretrained-bert\n",
        "\n",
        "# Install matplotlib-venn library for creating Venn diagrams using Matplotlib\n",
        "!pip install -q matplotlib-venn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install transformers library for working with pre-trained language models, including BERT\n",
        "!pip install -q transformers\n",
        "\n",
        "# Install torch library, which is PyTorch, a deep learning library for building and training neural networks\n",
        "!pip install -q torch\n"
      ],
      "metadata": {
        "id": "dsXoRdxLR7va"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the vocabulary file (vocab.txt) needed for BERT\n",
        "!wget https://raw.githubusercontent.com/Pranam2002/NER-on-Resume-using-BERT/main/vocab.txt\n",
        "\n",
        "# Downloading the resumes data file (Resumes.json) for training and evaluation\n",
        "!wget https://raw.githubusercontent.com/Pranam2002/NER-on-Resume-using-BERT/main/Resumes.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRvmS3pWSC72",
        "outputId": "fda700aa-8817-40ed-d82b-07204edd475c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-12 12:09:19--  https://raw.githubusercontent.com/Pranam2002/NER-on-Resume-using-BERT/main/vocab.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 231508 (226K) [text/plain]\n",
            "Saving to: ‘vocab.txt.1’\n",
            "\n",
            "\rvocab.txt.1           0%[                    ]       0  --.-KB/s               \rvocab.txt.1         100%[===================>] 226.08K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-01-12 12:09:19 (7.00 MB/s) - ‘vocab.txt.1’ saved [231508/231508]\n",
            "\n",
            "--2024-01-12 12:09:19--  https://raw.githubusercontent.com/Pranam2002/NER-on-Resume-using-BERT/main/Resumes.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1220776 (1.2M) [text/plain]\n",
            "Saving to: ‘Resumes.json.1’\n",
            "\n",
            "Resumes.json.1      100%[===================>]   1.16M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-01-12 12:09:19 (19.9 MB/s) - ‘Resumes.json.1’ saved [1220776/1220776]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking out the data from the json we have"
      ],
      "metadata": {
        "id": "Ff2NRIBcSvWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data_path = \"Resumes.json\"\n",
        "\n",
        "# Read the json file using pandas\n",
        "data_df = pd.read_json(data_path, lines = True)"
      ],
      "metadata": {
        "id": "vA8w1xdnSu92"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Showing first 10 records of the data\n",
        "data_df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "nDFPmkCHTIfH",
        "outputId": "14eecc0d-2c38-44bf-d4d8-85313f6045e1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             content  \\\n",
              "0  Abhishek Jha\\nApplication Development Associat...   \n",
              "1  Afreen Jamadar\\nActive member of IIIT Committe...   \n",
              "2  Akhil Yadav Polemaina\\nHyderabad, Telangana - ...   \n",
              "3  Alok Khandai\\nOperational Analyst (SQL DBA) En...   \n",
              "4  Ananya Chavan\\nlecturer - oracle tutorials\\n\\n...   \n",
              "5  Anvitha Rao\\nAutomation developer\\n\\n- Email m...   \n",
              "6  arjun ks\\nSenior Program coordinator - oracle ...   \n",
              "7  Arun Elumalai\\nQA Tester\\n\\nChennai, Tamil Nad...   \n",
              "8  Ashalata Bisoyi\\nTransaction Processor - Oracl...   \n",
              "9  Ashok Kunam\\nTeam Lead - Microsoft\\n\\n- Email ...   \n",
              "\n",
              "                                          annotation  extras  \n",
              "0  [{'label': ['Skills'], 'points': [{'start': 12...     NaN  \n",
              "1  [{'label': ['Email Address'], 'points': [{'sta...     NaN  \n",
              "2  [{'label': ['Skills'], 'points': [{'start': 37...     NaN  \n",
              "3  [{'label': ['Skills'], 'points': [{'start': 80...     NaN  \n",
              "4  [{'label': ['Degree'], 'points': [{'start': 20...     NaN  \n",
              "5  [{'label': ['Skills'], 'points': [{'start': 28...     NaN  \n",
              "6  [{'label': ['Skills'], 'points': [{'start': 34...     NaN  \n",
              "7  [{'label': ['Skills'], 'points': [{'start': 19...     NaN  \n",
              "8  [{'label': ['Skills'], 'points': [{'start': 17...     NaN  \n",
              "9  [{'label': ['Skills'], 'points': [{'start': 41...     NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f91be4ab-f7fb-4952-87bd-c3b7e2455188\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>annotation</th>\n",
              "      <th>extras</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Abhishek Jha\\nApplication Development Associat...</td>\n",
              "      <td>[{'label': ['Skills'], 'points': [{'start': 12...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Afreen Jamadar\\nActive member of IIIT Committe...</td>\n",
              "      <td>[{'label': ['Email Address'], 'points': [{'sta...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Akhil Yadav Polemaina\\nHyderabad, Telangana - ...</td>\n",
              "      <td>[{'label': ['Skills'], 'points': [{'start': 37...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Alok Khandai\\nOperational Analyst (SQL DBA) En...</td>\n",
              "      <td>[{'label': ['Skills'], 'points': [{'start': 80...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ananya Chavan\\nlecturer - oracle tutorials\\n\\n...</td>\n",
              "      <td>[{'label': ['Degree'], 'points': [{'start': 20...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Anvitha Rao\\nAutomation developer\\n\\n- Email m...</td>\n",
              "      <td>[{'label': ['Skills'], 'points': [{'start': 28...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>arjun ks\\nSenior Program coordinator - oracle ...</td>\n",
              "      <td>[{'label': ['Skills'], 'points': [{'start': 34...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Arun Elumalai\\nQA Tester\\n\\nChennai, Tamil Nad...</td>\n",
              "      <td>[{'label': ['Skills'], 'points': [{'start': 19...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Ashalata Bisoyi\\nTransaction Processor - Oracl...</td>\n",
              "      <td>[{'label': ['Skills'], 'points': [{'start': 17...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Ashok Kunam\\nTeam Lead - Microsoft\\n\\n- Email ...</td>\n",
              "      <td>[{'label': ['Skills'], 'points': [{'start': 41...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f91be4ab-f7fb-4952-87bd-c3b7e2455188')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f91be4ab-f7fb-4952-87bd-c3b7e2455188 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f91be4ab-f7fb-4952-87bd-c3b7e2455188');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b74ec8b3-5db8-4a05-8956-8a6ec00e9d07\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b74ec8b3-5db8-4a05-8956-8a6ec00e9d07')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b74ec8b3-5db8-4a05-8956-8a6ec00e9d07 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81SsQAW-TIdC",
        "outputId": "4dea5adf-4ab5-4c5a-b7ad-1dcd2271c879"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "content       Abhishek Jha\\nApplication Development Associat...\n",
              "annotation    [{'label': ['Skills'], 'points': [{'start': 12...\n",
              "extras                                                      NaN\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['content'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "VOKrh12HVCDA",
        "outputId": "588e7b1a-5268-4e2b-ab3a-667ac8d00768"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Abhishek Jha\\nApplication Development Associate - Accenture\\n\\nBengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\\n\\n• To work for an organization which provides me the opportunity to improve my skills\\nand knowledge for my individual and company's growth in best possible ways.\\n\\nWilling to relocate to: Bangalore, Karnataka\\n\\nWORK EXPERIENCE\\n\\nApplication Development Associate\\n\\nAccenture -\\n\\nNovember 2017 to Present\\n\\nRole: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries\\nfor the Bot which will be triggered based on given input. Also, Training the bot for different possible\\nutterances (Both positive and negative), which will be given as\\ninput by the user.\\n\\nEDUCATION\\n\\nB.E in Information science and engineering\\n\\nB.v.b college of engineering and technology -  Hubli, Karnataka\\n\\nAugust 2013 to June 2017\\n\\n12th in Mathematics\\n\\nWoodbine modern school\\n\\nApril 2011 to March 2013\\n\\n10th\\n\\nKendriya Vidyalaya\\n\\nApril 2001 to March 2011\\n\\nSKILLS\\n\\nC (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year),\\nDatabase Management System (Less than 1 year), Java (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTechnical Skills\\n\\nhttps://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN\\n\\n\\n• Programming language: C, C++, Java\\n• Oracle PeopleSoft\\n• Internet Of Things\\n• Machine Learning\\n• Database Management System\\n• Computer Networks\\n• Operating System worked on: Linux, Windows, Mac\\n\\nNon - Technical Skills\\n\\n• Honest and Hard-Working\\n• Tolerant and Flexible to Different Situations\\n• Polite and Calm\\n• Team-Player\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['annotation'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJqGwgMSUm0o",
        "outputId": "7c031e96-4c48-49c3-99d0-897edd851d77"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': ['Skills'],\n",
              "  'points': [{'start': 1295,\n",
              "    'end': 1621,\n",
              "    'text': '\\n• Programming language: C, C++, Java\\n• Oracle PeopleSoft\\n• Internet Of Things\\n• Machine Learning\\n• Database Management System\\n• Computer Networks\\n• Operating System worked on: Linux, Windows, Mac\\n\\nNon - Technical Skills\\n\\n• Honest and Hard-Working\\n• Tolerant and Flexible to Different Situations\\n• Polite and Calm\\n• Team-Player'}]},\n",
              " {'label': ['Skills'],\n",
              "  'points': [{'start': 993,\n",
              "    'end': 1153,\n",
              "    'text': 'C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year),\\nDatabase Management System (Less than 1 year), Java (Less than 1 year)'}]},\n",
              " {'label': ['College Name'],\n",
              "  'points': [{'start': 939, 'end': 956, 'text': 'Kendriya Vidyalaya'}]},\n",
              " {'label': ['College Name'],\n",
              "  'points': [{'start': 883, 'end': 904, 'text': 'Woodbine modern school'}]},\n",
              " {'label': ['Graduation Year'],\n",
              "  'points': [{'start': 856, 'end': 860, 'text': '2017\\n'}]},\n",
              " {'label': ['College Name'],\n",
              "  'points': [{'start': 771,\n",
              "    'end': 813,\n",
              "    'text': 'B.v.b college of engineering and technology'}]},\n",
              " {'label': ['Designation'],\n",
              "  'points': [{'start': 727,\n",
              "    'end': 769,\n",
              "    'text': 'B.E in Information science and engineering\\n'}]},\n",
              " {'label': ['Companies worked at'],\n",
              "  'points': [{'start': 407, 'end': 415, 'text': 'Accenture'}]},\n",
              " {'label': ['Designation'],\n",
              "  'points': [{'start': 372,\n",
              "    'end': 404,\n",
              "    'text': 'Application Development Associate'}]},\n",
              " {'label': ['Email Address'],\n",
              "  'points': [{'start': 95,\n",
              "    'end': 145,\n",
              "    'text': 'Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\\n'}]},\n",
              " {'label': ['Location'],\n",
              "  'points': [{'start': 60, 'end': 68, 'text': 'Bengaluru'}]},\n",
              " {'label': ['Companies worked at'],\n",
              "  'points': [{'start': 49, 'end': 57, 'text': 'Accenture'}]},\n",
              " {'label': ['Designation'],\n",
              "  'points': [{'start': 13,\n",
              "    'end': 45,\n",
              "    'text': 'Application Development Associate'}]},\n",
              " {'label': ['Name'],\n",
              "  'points': [{'start': 0, 'end': 11, 'text': 'Abhishek Jha'}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking out the shape and count of the records\n",
        "print(f\"Shape of the given data: {data_df.shape}\")\n",
        "print(f\"Count of records in the json data: {len(data_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psoAqPW_TS1n",
        "outputId": "9a1f848f-a18b-40f5-e988-fb34ef826fe5"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the given data: (220, 3)\n",
            "Count of records in the json data: 220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making utility functions for further use"
      ],
      "metadata": {
        "id": "6eFYJM-vTv61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the regular expression module for pattern matching\n",
        "import re\n",
        "\n",
        "# Import the JSON module for working with JSON data\n",
        "import json\n",
        "\n",
        "# Import the logging module for logging messages\n",
        "import logging\n",
        "\n",
        "# Import the NumPy library for numerical operations\n",
        "import numpy as np\n",
        "\n",
        "# Import the 'trange' function from tqdm for creating a progress bar during loops\n",
        "from tqdm import trange\n",
        "\n",
        "# Import the PyTorch library for deep learning\n",
        "import torch\n",
        "\n",
        "# Import specific components from PyTorch for building datasets and models\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Import the 'classification_report' function from seqeval for evaluating sequence labeling\n",
        "from seqeval.metrics import classification_report\n",
        "\n",
        "# Import the 'confusion_matrix' function from scikit-learn for evaluating classification performance\n",
        "from sklearn.metrics import confusion_matrix\n"
      ],
      "metadata": {
        "id": "lU6FfAnzTpuW"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_goldparse(dataturks_JSON_FilePath):\n",
        "    try:\n",
        "        # Initialize an empty list to store training data\n",
        "        training_data = []\n",
        "        # Initialize an empty list to store lines from the JSON file\n",
        "        lines = []\n",
        "\n",
        "        # Read lines from the JSON file\n",
        "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        # Iterate through each line in the JSON file\n",
        "        for line in lines:\n",
        "            # Load JSON data from the line\n",
        "            data = json.loads(line)\n",
        "            # Extract text content from the JSON data and replace newline characters with spaces\n",
        "            text = data['content'].replace(\"\\n\", \" \")\n",
        "            # Initialize an empty list to store entities for each text entry\n",
        "            entities = []\n",
        "\n",
        "            # Check if 'annotation' key exists in the JSON data\n",
        "            data_annotations = data['annotation']\n",
        "            if data_annotations is not None:\n",
        "                # Iterate through each annotation in the 'annotation' list\n",
        "                for annotation in data_annotations:\n",
        "                    # Extract information about the annotation\n",
        "                    point = annotation['points'][0]\n",
        "                    labels = annotation['label']\n",
        "\n",
        "                    # Ensure that labels is a list\n",
        "                    if not isinstance(labels, list):\n",
        "                        labels = [labels]\n",
        "\n",
        "                    # Iterate through each label in the list of labels\n",
        "                    for label in labels:\n",
        "                        point_start = point['start']\n",
        "                        point_end = point['end']\n",
        "                        point_text = point['text']\n",
        "\n",
        "                        # Adjust start and end positions based on leading and trailing whitespaces\n",
        "                        lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
        "                        rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
        "                        if lstrip_diff != 0:\n",
        "                            point_start = point_start + lstrip_diff\n",
        "                        if rstrip_diff != 0:\n",
        "                            point_end = point_end - rstrip_diff\n",
        "\n",
        "                        # Append the entity information to the list of entities\n",
        "                        entities.append((point_start, point_end + 1, label))\n",
        "\n",
        "            # Append the text and entities to the training data list\n",
        "            training_data.append((text, {\"entities\": entities}))\n",
        "\n",
        "        # Return the processed training data\n",
        "        return training_data\n",
        "\n",
        "    except Exception as e:\n",
        "        # Log an exception if there's an error processing the file\n",
        "        logging.exception(\"Unable to process \" +\n",
        "                          dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
        "        # Return None if there's an error\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "ihLsEJKtTtQI"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trim_entity_spans(data: list) -> list:\n",
        "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
        "\n",
        "    Args:\n",
        "        data (list): The data to be cleaned in spaCy JSON format.\n",
        "\n",
        "    Returns:\n",
        "        list: The cleaned data.\n",
        "    \"\"\"\n",
        "    # Define a regular expression to match white spaces\n",
        "    invalid_span_tokens = re.compile(r'\\s')\n",
        "\n",
        "    # Initialize an empty list to store cleaned data\n",
        "    cleaned_data = []\n",
        "\n",
        "    # Iterate through each entry in the input data\n",
        "    for text, annotations in data:\n",
        "        # Extract the entities from the annotations\n",
        "        entities = annotations['entities']\n",
        "        # Initialize an empty list to store valid entities\n",
        "        valid_entities = []\n",
        "\n",
        "        # Iterate through each entity in the list of entities\n",
        "        for start, end, label in entities:\n",
        "            # Initialize valid_start and valid_end with the original start and end positions\n",
        "            valid_start = start\n",
        "            valid_end = end\n",
        "\n",
        "            # Remove leading white spaces from the entity span\n",
        "            while valid_start < len(text) and invalid_span_tokens.match(text[valid_start]):\n",
        "                valid_start += 1\n",
        "\n",
        "            # Remove trailing white spaces from the entity span\n",
        "            while valid_end > 1 and invalid_span_tokens.match(text[valid_end - 1]):\n",
        "                valid_end -= 1\n",
        "\n",
        "            # Append the cleaned entity information to the list of valid entities\n",
        "            valid_entities.append([valid_start, valid_end, label])\n",
        "\n",
        "        # Append the text and cleaned entities to the cleaned data list\n",
        "        cleaned_data.append([text, {'entities': valid_entities}])\n",
        "\n",
        "    # Return the cleaned data\n",
        "    return cleaned_data\n"
      ],
      "metadata": {
        "id": "aL84Fr30TwjT"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label(offset, labels):\n",
        "    \"\"\"\n",
        "    Get the label for a given offset within a sequence of labels.\n",
        "\n",
        "    Args:\n",
        "        offset (tuple): A tuple representing the start and end positions (offset[0] and offset[1]) of the current span.\n",
        "        labels (list): A list of labels, each represented as a tuple (start, end, label).\n",
        "\n",
        "    Returns:\n",
        "        str: The label associated with the given offset.\n",
        "    \"\"\"\n",
        "    # Check if the offset represents a special case (e.g., no entity)\n",
        "    if offset[0] == 0 and offset[1] == 0:\n",
        "        return 'O'\n",
        "\n",
        "    # Iterate through each label in the list of labels\n",
        "    for label in labels:\n",
        "        # Check if the offset is within the boundaries of the current label\n",
        "        if offset[1] >= label[0] and offset[0] <= label[1]:\n",
        "            return label[2]  # Return the label associated with the offset\n",
        "\n",
        "    # Return 'O' if the offset does not belong to any labeled entity\n",
        "    return 'O'\n"
      ],
      "metadata": {
        "id": "keS5HJqoTytG"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of tag values (labels)\n",
        "tags_vals = [\"UNKNOWN\", \"O\", \"Name\", \"Degree\", \"Skills\", \"College Name\", \"Email Address\",\n",
        "             \"Designation\", \"Companies worked at\", \"Graduation Year\", \"Years of Experience\", \"Location\"]\n",
        "\n",
        "# Create a dictionary (tag2idx) mapping each tag to its index\n",
        "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
        "print(tag2idx)\n",
        "\n",
        "# Create a reverse dictionary (idx2tag) mapping each index to its corresponding tag\n",
        "idx2tag = {i: t for i, t in enumerate(tags_vals)}\n",
        "print(idx2tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEAGx_e7T0xk",
        "outputId": "d7a442fa-b7c6-481c-85af-168330cac18f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'UNKNOWN': 0, 'O': 1, 'Name': 2, 'Degree': 3, 'Skills': 4, 'College Name': 5, 'Email Address': 6, 'Designation': 7, 'Companies worked at': 8, 'Graduation Year': 9, 'Years of Experience': 10, 'Location': 11}\n",
            "{0: 'UNKNOWN', 1: 'O', 2: 'Name', 3: 'Degree', 4: 'Skills', 5: 'College Name', 6: 'Email Address', 7: 'Designation', 8: 'Companies worked at', 9: 'Graduation Year', 10: 'Years of Experience', 11: 'Location'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_resume(data, tokenizer, tag2idx, max_len, is_test=False):\n",
        "    \"\"\"\n",
        "    Process a resume by tokenizing, encoding input, and converting labels to numerical indices.\n",
        "\n",
        "    Args:\n",
        "        data (tuple): Tuple containing text data and annotations (entities) for a resume.\n",
        "        tokenizer: Tokenizer used for encoding the input.\n",
        "        tag2idx (dict): Dictionary mapping tags to their corresponding indices.\n",
        "        max_len (int): Maximum length for padding or truncating sequences.\n",
        "        is_test (bool): Flag indicating whether it is a test case.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing processed information for the resume.\n",
        "    \"\"\"\n",
        "    # Tokenize the input text using the provided tokenizer\n",
        "    tok = tokenizer.encode_plus(\n",
        "        data[0], max_length=max_len, return_offsets_mapping=True)\n",
        "\n",
        "    # Initialize a dictionary to store information for the current sentence\n",
        "    curr_sent = {'orig_labels': [], 'labels': []}\n",
        "\n",
        "    # Calculate padding length based on the difference between max_len and the length of input_ids\n",
        "    padding_length = max_len - len(tok['input_ids'])\n",
        "\n",
        "    # Check if it is not a test case\n",
        "    if not is_test:\n",
        "        # Extract labels from the annotations in reverse order\n",
        "        labels = data[1]['entities']\n",
        "        labels.reverse()\n",
        "\n",
        "        # Iterate through each offset in the offset_mapping\n",
        "        for off in tok['offset_mapping']:\n",
        "            # Get the label for the current offset using the get_label function\n",
        "            label = get_label(off, labels)\n",
        "\n",
        "            # Append the original label and its numerical index to the corresponding lists in curr_sent\n",
        "            curr_sent['orig_labels'].append(label)\n",
        "            curr_sent['labels'].append(tag2idx[label])\n",
        "\n",
        "        # Pad the labels with 0s to match the length of max_len\n",
        "        curr_sent['labels'] = curr_sent['labels'] + ([0] * padding_length)\n",
        "\n",
        "    # Pad input_ids, token_type_ids, and attention_mask to match the length of max_len\n",
        "    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n",
        "    curr_sent['token_type_ids'] = tok['token_type_ids'] + ([0] * padding_length)\n",
        "    curr_sent['attention_mask'] = tok['attention_mask'] + ([0] * padding_length)\n",
        "\n",
        "    # Return the processed information for the resume\n",
        "    return curr_sent\n"
      ],
      "metadata": {
        "id": "ghVsMPyWT2k-"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResumeDataset(Dataset):\n",
        "    def __init__(self, resume, tokenizer, tag2idx, max_len, is_test=False):\n",
        "        \"\"\"\n",
        "        Custom PyTorch Dataset class for handling resume data.\n",
        "\n",
        "        Args:\n",
        "            resume (list): List of resume data, each entry being a tuple of text and annotations.\n",
        "            tokenizer: Tokenizer used for encoding the input.\n",
        "            tag2idx (dict): Dictionary mapping tags to their corresponding indices.\n",
        "            max_len (int): Maximum length for padding or truncating sequences.\n",
        "            is_test (bool): Flag indicating whether it is a test case.\n",
        "        \"\"\"\n",
        "        self.resume = resume\n",
        "        self.tokenizer = tokenizer\n",
        "        self.is_test = is_test\n",
        "        self.tag2idx = tag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Get the total number of resumes in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Total number of resumes.\n",
        "        \"\"\"\n",
        "        return len(self.resume)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a single item (resume) from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the resume in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary containing processed information for the resume.\n",
        "        \"\"\"\n",
        "        # Process the resume using the process_resume function\n",
        "        data = process_resume(\n",
        "            self.resume[idx], self.tokenizer, self.tag2idx, self.max_len, self.is_test)\n",
        "\n",
        "        # Return a dictionary with tensor representations of input data and labels\n",
        "        return {\n",
        "            'input_ids': torch.tensor(data['input_ids'], dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(data['token_type_ids'], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(data['attention_mask'], dtype=torch.long),\n",
        "            'labels': torch.tensor(data['labels'], dtype=torch.long),\n",
        "            'orig_label': data['orig_labels']\n",
        "        }\n"
      ],
      "metadata": {
        "id": "Y4T3RxqzT4cM"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hyperparameters(model, ff):\n",
        "    \"\"\"\n",
        "    Get hyperparameters for optimizer based on full fine-tuning or partial fine-tuning.\n",
        "\n",
        "    Args:\n",
        "        model: The neural network model.\n",
        "        ff (bool): Flag indicating whether full fine-tuning is applied.\n",
        "\n",
        "    Returns:\n",
        "        list: List of dictionaries containing optimizer group parameters.\n",
        "    \"\"\"\n",
        "    # ff: full_finetuning\n",
        "    if ff:\n",
        "        # Get all named parameters of the model\n",
        "        param_optimizer = list(model.named_parameters())\n",
        "        # Define a list of parameter names that do not have weight decay\n",
        "        no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
        "        # Group parameters into two groups: those with weight decay and those without\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
        "                ],\n",
        "                \"weight_decay_rate\": 0.01,  # Set weight decay rate for the first group\n",
        "            },\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
        "                ],\n",
        "                \"weight_decay_rate\": 0.0,  # Set weight decay rate for the second group\n",
        "            },\n",
        "        ]\n",
        "    else:\n",
        "        # Get named parameters of the classifier (assuming there's a classifier in the model)\n",
        "        param_optimizer = list(model.classifier.named_parameters())\n",
        "        # Group all classifier parameters into a single group\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\"params\": [p for n, p in param_optimizer]}]\n",
        "\n",
        "    # Return the list of optimizer group parameters\n",
        "    return optimizer_grouped_parameters\n"
      ],
      "metadata": {
        "id": "nH28fbTnT6UO"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_special_tokens(tokenizer, tag2idx):\n",
        "    \"\"\"\n",
        "    Get token IDs for special tokens and the index of the \"O\" label.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: Tokenizer from which to obtain special token IDs.\n",
        "        tag2idx (dict): Dictionary mapping tags to their corresponding indices.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Tuple containing token IDs for [PAD], [SEP], [CLS], and the index of \"O\" label.\n",
        "    \"\"\"\n",
        "    # Get the vocabulary from the tokenizer\n",
        "    vocab = tokenizer.get_vocab()\n",
        "\n",
        "    # Obtain the token IDs for special tokens\n",
        "    pad_tok = vocab[\"[PAD]\"]\n",
        "    sep_tok = vocab[\"[SEP]\"]\n",
        "    cls_tok = vocab[\"[CLS]\"]\n",
        "\n",
        "    # Get the index of the \"O\" label from the tag2idx mapping\n",
        "    o_lab = tag2idx[\"O\"]\n",
        "\n",
        "    # Return the obtained token IDs and label index\n",
        "    return pad_tok, sep_tok, cls_tok, o_lab\n"
      ],
      "metadata": {
        "id": "dDPzAXNZT8bB"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def annot_confusion_matrix(valid_tags, pred_tags):\n",
        "    \"\"\"\n",
        "    Create an annotated confusion matrix by adding label annotations and formatting\n",
        "    to sklearn's `confusion_matrix`.\n",
        "\n",
        "    Args:\n",
        "        valid_tags (list): List of true labels.\n",
        "        pred_tags (list): List of predicted labels.\n",
        "\n",
        "    Returns:\n",
        "        str: Annotated confusion matrix content.\n",
        "    \"\"\"\n",
        "    # Get unique labels from both true and predicted labels\n",
        "    header = sorted(list(set(valid_tags + pred_tags)))\n",
        "\n",
        "    # Compute the confusion matrix using sklearn's `confusion_matrix`\n",
        "    matrix = confusion_matrix(valid_tags, pred_tags, labels=header)\n",
        "\n",
        "    # Format the confusion matrix with label annotations\n",
        "    mat_formatted = [header[i] + \"\\t\\t\\t\" + str(row) for i, row in enumerate(matrix)]\n",
        "    content = \"\\t\" + \" \".join(header) + \"\\n\" + \"\\n\".join(mat_formatted)\n",
        "\n",
        "    # Return the annotated confusion matrix content\n",
        "    return content\n"
      ],
      "metadata": {
        "id": "VHQoivTzT-Ay"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flat_accuracy(valid_tags, pred_tags):\n",
        "    \"\"\"\n",
        "    Calculate flat accuracy, which is the proportion of correct predictions\n",
        "    in a flat (non-sequence) setting.\n",
        "\n",
        "    Args:\n",
        "        valid_tags (list): List of true labels.\n",
        "        pred_tags (list): List of predicted labels.\n",
        "\n",
        "    Returns:\n",
        "        float: Flat accuracy.\n",
        "    \"\"\"\n",
        "    # Convert lists to NumPy arrays and calculate mean of element-wise equality\n",
        "    accuracy = (np.array(valid_tags) == np.array(pred_tags)).mean()\n",
        "\n",
        "    # Return the calculated flat accuracy\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "ZF6hLXfLT_y8"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_val_model(\n",
        "    model,\n",
        "    TOKENIZER,\n",
        "    optimizer,\n",
        "    EPOCHS,\n",
        "    idx2tag,\n",
        "    tag2idx,\n",
        "    MAX_GRAD_NORM,\n",
        "    DEVICE,\n",
        "    train_dataloader,\n",
        "    valid_dataloader\n",
        "):\n",
        "\n",
        "    pad_tok, sep_tok, cls_tok, o_lab = get_special_tokens(TOKENIZER, tag2idx)\n",
        "\n",
        "    epoch = 0\n",
        "    for _ in trange(EPOCHS, desc=\"Epoch\"):\n",
        "        epoch += 1\n",
        "\n",
        "        # Training loop\n",
        "        print(\"Starting training loop.\")\n",
        "        model.train()\n",
        "        tr_loss, tr_accuracy = 0, 0\n",
        "        nb_tr_examples, nb_tr_steps = 0, 0\n",
        "        tr_preds, tr_labels = [], []\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            # Add batch to gpu\n",
        "\n",
        "            # batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
        "            b_input_ids, b_input_mask, b_labels = b_input_ids.to(\n",
        "                DEVICE), b_input_mask.to(DEVICE), b_labels.to(DEVICE)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                b_input_ids,\n",
        "                token_type_ids=None,\n",
        "                attention_mask=b_input_mask,\n",
        "                labels=b_labels,\n",
        "            )\n",
        "            loss, tr_logits = outputs[:2]\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Compute train loss\n",
        "            tr_loss += loss.item()\n",
        "            nb_tr_examples += b_input_ids.size(0)\n",
        "            nb_tr_steps += 1\n",
        "\n",
        "            # Subset out unwanted predictions on CLS/PAD/SEP tokens\n",
        "            preds_mask = (\n",
        "                (b_input_ids != cls_tok)\n",
        "                & (b_input_ids != pad_tok)\n",
        "                & (b_input_ids != sep_tok)\n",
        "            )\n",
        "\n",
        "            tr_logits = tr_logits.cpu().detach().numpy()\n",
        "            tr_label_ids = torch.masked_select(b_labels, (preds_mask == 1))\n",
        "            preds_mask = preds_mask.cpu().detach().numpy()\n",
        "            tr_batch_preds = np.argmax(tr_logits[preds_mask.squeeze()], axis=1)\n",
        "            tr_batch_labels = tr_label_ids.to(\"cpu\").numpy()\n",
        "            tr_preds.extend(tr_batch_preds)\n",
        "            tr_labels.extend(tr_batch_labels)\n",
        "\n",
        "            # Compute training accuracy\n",
        "            tmp_tr_accuracy = flat_accuracy(tr_batch_labels, tr_batch_preds)\n",
        "            tr_accuracy += tmp_tr_accuracy\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "            )\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "            model.zero_grad()\n",
        "\n",
        "        tr_loss = tr_loss / nb_tr_steps\n",
        "        tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "\n",
        "        # Print training loss and accuracy per epoch\n",
        "        print(f\"Train loss: {tr_loss}\")\n",
        "        print(f\"Train accuracy: {tr_accuracy}\")\n",
        "\n",
        "        \"\"\"\n",
        "        Validation loop\n",
        "        \"\"\"\n",
        "        print(\"Starting validation loop.\")\n",
        "\n",
        "        model.eval()\n",
        "        eval_loss, eval_accuracy = 0, 0\n",
        "        nb_eval_steps, nb_eval_examples = 0, 0\n",
        "        predictions, true_labels = [], []\n",
        "\n",
        "        for batch in valid_dataloader:\n",
        "\n",
        "            b_input_ids, b_input_mask, b_labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
        "            b_input_ids, b_input_mask, b_labels = b_input_ids.to(\n",
        "                DEVICE), b_input_mask.to(DEVICE), b_labels.to(DEVICE)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(\n",
        "                    b_input_ids,\n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=b_input_mask,\n",
        "                    labels=b_labels,\n",
        "                )\n",
        "                tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "            # Subset out unwanted predictions on CLS/PAD/SEP tokens\n",
        "            preds_mask = (\n",
        "                (b_input_ids != cls_tok)\n",
        "                & (b_input_ids != pad_tok)\n",
        "                & (b_input_ids != sep_tok)\n",
        "            )\n",
        "\n",
        "            logits = logits.cpu().detach().numpy()\n",
        "            label_ids = torch.masked_select(b_labels, (preds_mask == 1))\n",
        "            preds_mask = preds_mask.cpu().detach().numpy()\n",
        "            val_batch_preds = np.argmax(logits[preds_mask.squeeze()], axis=1)\n",
        "            val_batch_labels = label_ids.to(\"cpu\").numpy()\n",
        "            predictions.extend(val_batch_preds)\n",
        "            true_labels.extend(val_batch_labels)\n",
        "\n",
        "            tmp_eval_accuracy = flat_accuracy(\n",
        "                val_batch_labels, val_batch_preds)\n",
        "\n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "            nb_eval_examples += b_input_ids.size(0)\n",
        "            nb_eval_steps += 1\n",
        "\n",
        "        # Evaluate loss, acc, conf. matrix, and class. report on devset\n",
        "        pred_tags = [idx2tag[i] for i in predictions]\n",
        "        valid_tags = [idx2tag[i] for i in true_labels]\n",
        "        cl_report = classification_report(valid_tags, pred_tags)\n",
        "        conf_mat = annot_confusion_matrix(valid_tags, pred_tags)\n",
        "        eval_loss = eval_loss / nb_eval_steps\n",
        "        eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "\n",
        "        # Report metrics\n",
        "        print(f\"Validation loss: {eval_loss}\")\n",
        "        print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "        print(f\"Classification Report:\\n {cl_report}\")\n",
        "        print(f\"Confusion Matrix:\\n {conf_mat}\")"
      ],
      "metadata": {
        "id": "2jHU8k2aUBn4"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and fine-tuning the BERT model using the utility functions"
      ],
      "metadata": {
        "id": "fLjOLb5VUFwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries and modules\n",
        "import numpy as np  # NumPy for numerical operations\n",
        "import torch  # PyTorch for deep learning\n",
        "from transformers import BertForTokenClassification, BertTokenizerFast  # Hugging Face's Transformers library for BERT\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler  # PyTorch utilities for data handling\n",
        "from torch.optim import Adam  # Adam optimizer for model training"
      ],
      "metadata": {
        "id": "CrV5ynt0UDL_"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the maximum length of input sequences\n",
        "MAX_LEN = 500\n",
        "\n",
        "# Set the number of training epochs\n",
        "EPOCHS = 5\n",
        "\n",
        "# Set the maximum gradient norm for gradient clipping during training\n",
        "MAX_GRAD_NORM = 1.0\n",
        "\n",
        "# Specify the pre-trained BERT model name to be used\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "\n",
        "# Initialize the BERT tokenizer with a custom vocabulary file ('vocab.txt') and lowercase option\n",
        "TOKENIZER = BertTokenizerFast('vocab.txt', lowercase=True)\n",
        "\n",
        "# Check for the available device (GPU if available, otherwise CPU)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Convert and preprocess the data by trimming entity spans and converting to the appropriate format\n",
        "data = trim_entity_spans(convert_goldparse('Resumes.json'))\n",
        "\n",
        "# Set the truncation flag to True (assuming this is for truncating sequences during tokenization)\n",
        "truncation = True"
      ],
      "metadata": {
        "id": "PgcrvZAxUGYD"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of data points\n",
        "total = len(data)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data, val_data = data[:180], data[180:]\n",
        "\n",
        "# Create instances of the ResumeDataset class for training and validation, using the defined parameters\n",
        "train_d = ResumeDataset(train_data, TOKENIZER, tag2idx, MAX_LEN)\n",
        "val_d = ResumeDataset(val_data, TOKENIZER, tag2idx, MAX_LEN)\n",
        "\n",
        "# Use RandomSampler for training data to sample batches randomly\n",
        "train_sampler = RandomSampler(train_d)\n",
        "\n",
        "# Create a DataLoader for the training set with a batch size of 8\n",
        "train_dl = DataLoader(train_d, batch_size=8, sampler=train_sampler)\n",
        "\n",
        "# Create a DataLoader for the validation set with a batch size of 8\n",
        "val_dl = DataLoader(val_d, batch_size=8)"
      ],
      "metadata": {
        "id": "swScL-xwUIYG"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dl), len(val_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VepjmRzVULAi",
        "outputId": "1f717c32-598c-474c-cb20-9735121eba9e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a BERT-based token classification model from pre-trained weights\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(tag2idx)\n",
        ")\n",
        "\n",
        "# Move the model to the specified device (GPU or CPU)\n",
        "model.to(DEVICE)\n",
        "\n",
        "# Get optimizer group parameters based on full fine-tuning and model parameters\n",
        "optimizer_grouped_parameters = get_hyperparameters(model, True)\n",
        "\n",
        "# Initialize the Adam optimizer with a learning rate of 3e-5\n",
        "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeMdbOSiUNQk",
        "outputId": "a67f5ec5-89ac-481f-face-b6561a00150d"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# ...\n",
        "\n",
        "# Define a collate function to handle padding of variable-length sequences in a batch\n",
        "def collate_fn(batch):\n",
        "    # Pad input_ids sequences in the batch\n",
        "    input_ids = pad_sequence([item['input_ids'] for item in batch], batch_first=True, padding_value=0)\n",
        "\n",
        "    # Pad token_type_ids sequences in the batch\n",
        "    token_type_ids = pad_sequence([item['token_type_ids'] for item in batch], batch_first=True, padding_value=0)\n",
        "\n",
        "    # Pad attention_mask sequences in the batch\n",
        "    attention_mask = pad_sequence([item['attention_mask'] for item in batch], batch_first=True, padding_value=0)\n",
        "\n",
        "    # Pad labels sequences in the batch\n",
        "    labels = pad_sequence([item['labels'] for item in batch], batch_first=True, padding_value=0)\n",
        "\n",
        "    # Gather original labels for later analysis or evaluation\n",
        "    orig_labels = [item['orig_label'] for item in batch]\n",
        "\n",
        "    # Return a dictionary containing the padded sequences and original labels\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'labels': labels,\n",
        "        'orig_label': orig_labels\n",
        "    }\n",
        "\n",
        "# ...\n",
        "\n",
        "# Create DataLoader instances for training and validation sets with the specified collate function\n",
        "train_dl = DataLoader(train_d, batch_size=8, collate_fn=collate_fn)\n",
        "val_dl = DataLoader(val_d, batch_size=8, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "0MDl3gHswIQx"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_val_model(\n",
        "    model,\n",
        "    TOKENIZER,\n",
        "    optimizer,\n",
        "    EPOCHS,\n",
        "    idx2tag,\n",
        "    tag2idx,\n",
        "    MAX_GRAD_NORM,\n",
        "    DEVICE,\n",
        "    train_dl,\n",
        "    val_dl\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wamVMBuUPAL",
        "outputId": "5ee368c2-deb0-48e7-9e1f-869389f43db6"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/5 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training loop.\n",
            "Train loss: 1.052687537410985\n",
            "Train accuracy: 0.7674854768429418\n",
            "Starting validation loop.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  20%|██        | 1/5 [00:22<01:29, 22.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.5996107280254364\n",
            "Validation Accuracy: 0.8292081734569011\n",
            "Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "      Email Address       0.88      0.62      0.73      1130\n",
            "             Skills       0.00      0.00      0.00       869\n",
            "Companies worked at       0.00      0.00      0.00        59\n",
            "             Degree       0.00      0.00      0.00        35\n",
            "        Designation       0.00      0.00      0.00        89\n",
            "           Location       0.00      0.00      0.00        54\n",
            "       College Name       0.00      0.00      0.00        33\n",
            "    Graduation Year       0.00      0.00      0.00        16\n",
            "               Name       0.00      0.00      0.00        41\n",
            "Years of Experience       0.00      0.00      0.00         5\n",
            "\n",
            "          micro avg       0.88      0.30      0.45      2331\n",
            "          macro avg       0.43      0.30      0.35      2331\n",
            "\n",
            "Confusion Matrix:\n",
            " \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills UNKNOWN Years of Experience\n",
            "College Name\t\t\t[  0   0   0   0   0   0   0   0 208   0   0   0]\n",
            "Companies worked at\t\t\t[  0   0   0   0   0   0   0   0 236   0   0   0]\n",
            "Degree\t\t\t[  0   0   0   0   0   0   0   0 152   0   0   0]\n",
            "Designation\t\t\t[  0   0   0   0   0   0   0   0 323   0   0   0]\n",
            "Email Address\t\t\t[  0   0   0   0 697   0   0   0 433   0   0   0]\n",
            "Graduation Year\t\t\t[ 0  0  0  0  0  0  0  0 19  0  0  0]\n",
            "Location\t\t\t[  0   0   0   0   0   0   0   0 175   0   0   0]\n",
            "Name\t\t\t[  0   0   0   0   3   0   0   0 187   0   0   0]\n",
            "O\t\t\t[    0     0     0     0    92     0     0     0 12615     0     1     0]\n",
            "Skills\t\t\t[  0   0   0   0   0   0   0   0 869   0   0   0]\n",
            "UNKNOWN\t\t\t[0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Years of Experience\t\t\t[ 0  0  0  0  0  0  0  0 15  0  0  0]\n",
            "Starting training loop.\n",
            "Train loss: 0.4711104780435562\n",
            "Train accuracy: 0.8581131937427378\n",
            "Starting validation loop.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  40%|████      | 2/5 [00:43<01:04, 21.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.37758908867836\n",
            "Validation Accuracy: 0.865977893411967\n",
            "Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "      Email Address       0.84      0.76      0.80      1130\n",
            "             Skills       0.98      0.27      0.42       869\n",
            "Companies worked at       0.00      0.00      0.00        60\n",
            "             Degree       0.00      0.00      0.00        35\n",
            "        Designation       0.11      0.04      0.06        92\n",
            "           Location       0.00      0.00      0.00        53\n",
            "       College Name       0.00      0.00      0.00        33\n",
            "    Graduation Year       0.00      0.00      0.00        16\n",
            "               Name       0.37      0.56      0.45        41\n",
            "Years of Experience       0.00      0.00      0.00         5\n",
            "\n",
            "          micro avg       0.80      0.48      0.60      2334\n",
            "          macro avg       0.78      0.48      0.55      2334\n",
            "\n",
            "Confusion Matrix:\n",
            " \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n",
            "College Name\t\t\t[ 10   0   0   0   0   0   0   0 198   0   0]\n",
            "Companies worked at\t\t\t[  0   0   0   0   2   0   1  15 227   0   0]\n",
            "Degree\t\t\t[ 16   0   1   7   0   0   0   1 127   0   0]\n",
            "Designation\t\t\t[  0   0   0  52   4   0   0  13 245   0   0]\n",
            "Email Address\t\t\t[  0   0   0   0 854   0   0   0 276   0   0]\n",
            "Graduation Year\t\t\t[ 0  0  0  0  0  0  0  0 19  0  0]\n",
            "Location\t\t\t[  0   0   0   0   5   0  19  13 138   0   0]\n",
            "Name\t\t\t[  0   0   0   0   0   0   0 181   9   0   0]\n",
            "O\t\t\t[    4     0     0     2   150     0     1     7 12539     5     0]\n",
            "Skills\t\t\t[  0   0   0   0   0   0   0   0 634 235   0]\n",
            "Years of Experience\t\t\t[ 0  0  0  0  1  0  0  0 14  0  0]\n",
            "Starting training loop.\n",
            "Train loss: 0.32013486552497616\n",
            "Train accuracy: 0.8891984904148325\n",
            "Starting validation loop.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  60%|██████    | 3/5 [01:02<00:41, 20.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.3109576880931854\n",
            "Validation Accuracy: 0.891132934312937\n",
            "Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "      Email Address       0.88      0.75      0.81      1130\n",
            "             Skills       0.92      0.41      0.57       869\n",
            "Companies worked at       0.15      0.12      0.13        59\n",
            "             Degree       0.08      0.06      0.07        35\n",
            "        Designation       0.28      0.22      0.25        89\n",
            "           Location       0.24      0.20      0.22        54\n",
            "       College Name       0.04      0.06      0.05        33\n",
            "    Graduation Year       0.00      0.00      0.00        16\n",
            "               Name       0.69      0.76      0.72        41\n",
            "Years of Experience       0.00      0.00      0.00         5\n",
            "\n",
            "          micro avg       0.78      0.55      0.64      2331\n",
            "          macro avg       0.80      0.55      0.64      2331\n",
            "\n",
            "Confusion Matrix:\n",
            " \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n",
            "College Name\t\t\t[104   0   2   0   0   0   1   0 101   0   0]\n",
            "Companies worked at\t\t\t[  2  62   0  12   0   0   2   3 155   0   0]\n",
            "Degree\t\t\t[22  0 68  0  0  0  0  0 62  0  0]\n",
            "Designation\t\t\t[  0  10   0 136   0   0   0   1 176   0   0]\n",
            "Email Address\t\t\t[  0   0   0   0 846   0   0   0 284   0   0]\n",
            "Graduation Year\t\t\t[ 0  0  0  0  0  0  0  0 19  0  0]\n",
            "Location\t\t\t[  0   0   0   0   0   0  71   4 100   0   0]\n",
            "Name\t\t\t[  0   0   0   4   0   0   0 182   4   0   0]\n",
            "O\t\t\t[   34    23     1    19   119     0     8     2 12472    30     0]\n",
            "Skills\t\t\t[  0   0   0   0   0   0   0   0 513 356   0]\n",
            "Years of Experience\t\t\t[ 0  0  0  1  0  0  0  0 14  0  0]\n",
            "Starting training loop.\n",
            "Train loss: 0.23569962641467218\n",
            "Train accuracy: 0.9175582777018502\n",
            "Starting validation loop.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  80%|████████  | 4/5 [01:23<00:20, 20.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.292404967546463\n",
            "Validation Accuracy: 0.8891514744117094\n",
            "Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "      Email Address       0.88      0.74      0.80      1130\n",
            "             Skills       0.62      0.67      0.64       869\n",
            "Companies worked at       0.18      0.32      0.23        60\n",
            "             Degree       0.26      0.23      0.24        35\n",
            "        Designation       0.21      0.20      0.20        92\n",
            "           Location       0.60      0.60      0.60        53\n",
            "       College Name       0.17      0.30      0.22        33\n",
            "    Graduation Year       0.00      0.00      0.00        16\n",
            "               Name       0.79      0.83      0.81        41\n",
            "Years of Experience       0.00      0.00      0.00         5\n",
            "\n",
            "          micro avg       0.68      0.66      0.67      2334\n",
            "          macro avg       0.70      0.66      0.68      2334\n",
            "\n",
            "Confusion Matrix:\n",
            " \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n",
            "College Name\t\t\t[159   0   2   0   0   0   1   0  46   0   0]\n",
            "Companies worked at\t\t\t[  6 138   0  10   0   0   2   3  86   0   0]\n",
            "Degree\t\t\t[21  0 99  1  0  0  0  0 31  0  0]\n",
            "Designation\t\t\t[  0  43   0 179   0   0   0   6  82   4   0]\n",
            "Email Address\t\t\t[  0   0   0   0 840   0   0   1 289   0   0]\n",
            "Graduation Year\t\t\t[ 0  0  0  0  0  0  0  0 19  0  0]\n",
            "Location\t\t\t[  0   0   0   0   0   0 110   0  65   0   0]\n",
            "Name\t\t\t[  0   0   0   1   0   0   0 186   3   0   0]\n",
            "O\t\t\t[   59    91     4    51   119     0    32     1 11989   362     0]\n",
            "Skills\t\t\t[  0   0   1   0   0   0   0   0 283 585   0]\n",
            "Years of Experience\t\t\t[ 0  0  0  3  0  0  0  0 12  0  0]\n",
            "Starting training loop.\n",
            "Train loss: 0.1839490830898285\n",
            "Train accuracy: 0.9347131715681707\n",
            "Starting validation loop.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 5/5 [01:43<00:00, 20.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.302430784702301\n",
            "Validation Accuracy: 0.8989736792367224\n",
            "Classification Report:\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "      Email Address       0.88      0.73      0.80      1130\n",
            "             Skills       0.96      0.41      0.58       869\n",
            "Companies worked at       0.14      0.24      0.18        59\n",
            "             Degree       0.48      0.34      0.40        35\n",
            "        Designation       0.19      0.17      0.18        89\n",
            "           Location       0.67      0.57      0.62        54\n",
            "       College Name       0.17      0.27      0.21        33\n",
            "    Graduation Year       0.00      0.00      0.00        16\n",
            "               Name       0.80      0.85      0.82        41\n",
            "Years of Experience       0.00      0.00      0.00         5\n",
            "\n",
            "          micro avg       0.78      0.56      0.65      2331\n",
            "          macro avg       0.83      0.56      0.65      2331\n",
            "\n",
            "Confusion Matrix:\n",
            " \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\n",
            "College Name\t\t\t[132   0   3   0   0   0   0   0  73   0   0]\n",
            "Companies worked at\t\t\t[  2 116   0   8   0   0   2   3 105   0   0]\n",
            "Degree\t\t\t[10  0 98  0  0  0  0  0 44  0  0]\n",
            "Designation\t\t\t[  0  45   0 172   0   0   0   5 101   0   0]\n",
            "Email Address\t\t\t[  0   0   0   0 830   0   0   0 300   0   0]\n",
            "Graduation Year\t\t\t[ 0  0  0  0  0  0  0  0 19  0  0]\n",
            "Location\t\t\t[  1   0   0   0   0   0 111   0  63   0   0]\n",
            "Name\t\t\t[  0   0   0   1   0   0   0 186   3   0   0]\n",
            "O\t\t\t[   32    68     0    32   117     0    22     2 12419    16     0]\n",
            "Skills\t\t\t[  0   0   0   1   0   0   0   0 510 358   0]\n",
            "Years of Experience\t\t\t[ 0  0  0  2  0  0  0  0 13  0  0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the state dictionary of the trained model to a file\n",
        "torch.save(\n",
        "    {\n",
        "        \"model_state_dict\": model.state_dict()\n",
        "    },\n",
        "    f'model-state-ner-resume.pt',\n",
        ")"
      ],
      "metadata": {
        "id": "_f0Dyv_DZ159"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making predictions using some sample resume data"
      ],
      "metadata": {
        "id": "3kP-8eSaXV3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyTorch for deep learning operations\n",
        "import torch\n",
        "\n",
        "# Import NumPy for numerical operations\n",
        "import numpy as np\n",
        "\n",
        "# Import textwrap for text formatting\n",
        "import textwrap\n"
      ],
      "metadata": {
        "id": "-Sgeos4_XWR2"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_resume(text, tokenizer, max_len):\n",
        "    # Tokenize the input text using the provided tokenizer and handle offset mappings\n",
        "    tok = tokenizer.encode_plus(\n",
        "        text, max_length=max_len, return_offsets_mapping=True)\n",
        "\n",
        "    # Initialize a dictionary to store tokenized information for the current sentence\n",
        "    curr_sent = dict()\n",
        "\n",
        "    # Calculate the padding length to ensure the output tensors have consistent lengths\n",
        "    padding_length = max_len - len(tok['input_ids'])\n",
        "\n",
        "    # Populate the dictionary with tokenized information and padding\n",
        "    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n",
        "    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n",
        "        ([0] * padding_length)\n",
        "    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n",
        "        ([0] * padding_length)\n",
        "\n",
        "    # Create a final dictionary containing tensors for input_ids, token_type_ids, attention_mask, and offset_mapping\n",
        "    final_data = {\n",
        "        'input_ids': torch.tensor(curr_sent['input_ids'], dtype=torch.long),\n",
        "        'token_type_ids': torch.tensor(curr_sent['token_type_ids'], dtype=torch.long),\n",
        "        'attention_mask': torch.tensor(curr_sent['attention_mask'], dtype=torch.long),\n",
        "        'offset_mapping': tok['offset_mapping']\n",
        "    }\n",
        "\n",
        "    # Return the final tokenized data\n",
        "    return final_data\n"
      ],
      "metadata": {
        "id": "8D45FjPaYfxB"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of entity tags\n",
        "tags_vals = [\"UNKNOWN\", \"O\", \"Name\", \"Degree\", \"Skills\", \"College Name\", \"Email Address\",\n",
        "             \"Designation\", \"Companies worked at\", \"Graduation Year\", \"Years of Experience\", \"Location\"]\n",
        "\n",
        "# Create a mapping from index to tag for easier reference\n",
        "idx2tag = {i: t for i, t in enumerate(tags_vals)}\n",
        "\n",
        "# Specify a list of restricted labels that are excluded from certain processing\n",
        "resticted_lables = [\"UNKNOWN\", \"O\", \"Email Address\"]\n"
      ],
      "metadata": {
        "id": "R6zeSRJ_Yfuq"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, tokenizer, idx2tag, device, test_resume, max_len):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the test resume using the provided tokenizer\n",
        "    data = tokenize_resume(test_resume, tokenizer, max_len)\n",
        "    input_ids, input_mask = data['input_ids'], data['attention_mask']\n",
        "\n",
        "    # Create labels tensor for the input_ids\n",
        "    labels = torch.tensor([1] * input_ids.size(0), dtype=torch.long)\n",
        "\n",
        "    # Move tensors to the specified device (GPU or CPU)\n",
        "    input_ids = input_ids.to(device)\n",
        "    input_mask = input_mask.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Disable gradient calculation during inference\n",
        "    with torch.no_grad():\n",
        "        # Perform inference on the model\n",
        "        outputs = model(\n",
        "            input_ids.unsqueeze(0),\n",
        "            token_type_ids=None,\n",
        "            attention_mask=input_mask.unsqueeze(0),\n",
        "            labels=labels.unsqueeze(0),\n",
        "        )\n",
        "        tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "    # Convert logits to NumPy array\n",
        "    logits = logits.cpu().detach().numpy()\n",
        "\n",
        "    # Find the predicted label ids with the highest probability along the last axis\n",
        "    label_ids = np.argmax(logits, axis=2)\n",
        "\n",
        "    # Process the predicted labels and offset mapping to extract entities\n",
        "    entities = []\n",
        "    for label_id, offset in zip(label_ids[0], data['offset_mapping']):\n",
        "        curr_id = idx2tag[label_id]\n",
        "        curr_start = offset[0]\n",
        "        curr_end = offset[1]\n",
        "\n",
        "        # Check if the predicted label is not in restricted labels\n",
        "        if curr_id not in resticted_lables:\n",
        "            # Check for overlapping entities and merge if needed\n",
        "            if len(entities) > 0 and entities[-1]['entity'] == curr_id and curr_start - entities[-1]['end'] in [0, 1]:\n",
        "                entities[-1]['end'] = curr_end\n",
        "            else:\n",
        "                entities.append(\n",
        "                    {'entity': curr_id, 'start': curr_start, 'end': curr_end})\n",
        "\n",
        "    # Add the text content for each entity\n",
        "    for ent in entities:\n",
        "        ent['text'] = test_resume[ent['start']:ent['end']]\n",
        "\n",
        "    # Return the list of extracted entities\n",
        "    return entities\n"
      ],
      "metadata": {
        "id": "TXZ3EsPJYfsb"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the maximum length for tokenization\n",
        "MAX_LEN = 500\n",
        "\n",
        "# Specify the number of labels for token classification\n",
        "NUM_LABELS = 12\n",
        "\n",
        "# Choose the device for model training (GPU if available, else CPU)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the pre-trained BERT model path\n",
        "MODEL_PATH = 'bert-base-uncased'\n",
        "\n",
        "# Load the saved model state dictionary from the file\n",
        "STATE_DICT = torch.load(\"model-state-ner-resume.pt\", map_location=DEVICE)\n",
        "\n",
        "# Create a BERT tokenizer using the specified vocabulary file and lowercase option\n",
        "TOKENIZER = BertTokenizerFast(\"vocab.txt\", lowercase=True)\n",
        "\n",
        "# Initialize a BERT-based token classification model using the loaded state dictionary\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    'bert-base-uncased', state_dict=STATE_DICT['model_state_dict'], num_labels=NUM_LABELS)\n",
        "\n",
        "# Move the model to the specified device (GPU or CPU)\n",
        "model.to(DEVICE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbomBmgSYfmF",
        "outputId": "96938842-973c-4fe4-a090-f49475c7eb3c"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=12, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Replace newline and formfeed characters with a space\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace(\"\\f\", \" \")\n",
        "\n",
        "    # Strip leading and trailing whitespaces\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "yXx8tubnbG13"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_text = \"\"\"\n",
        "Govardhana K\\nSenior Software Engineer\\n\\nBengaluru, Karnataka, Karnataka - Email me on Indeed: indeed.com/r/Govardhana-K/\\nb2de315d95905b68\\n\\nTotal IT experience 5 Years 6 Months\\nCloud Lending Solutions INC 4 Month • Salesforce Developer\\nOracle 5 Years 2 Month • Core Java Developer\\nLanguages Core Java, Go Lang\\nOracle PL-SQL programming,\\nSales Force Developer with APEX.\\n\\nDesignations & Promotions\\n\\nWilling to relocate: Anywhere\\n\\nWORK EXPERIENCE\\n\\nSenior Software Engineer\\n\\nCloud Lending Solutions -  Bangalore, Karnataka -\\n\\nJanuary 2018 to Present\\n\\nPresent\\n\\nSenior Consultant\\n\\nOracle -  Bangalore, Karnataka -\\n\\nNovember 2016 to December 2017\\n\\nStaff Consultant\\n\\nOracle -  Bangalore, Karnataka -\\n\\nJanuary 2014 to October 2016\\n\\nAssociate Consultant\\n\\nOracle -  Bangalore, Karnataka -\\n\\nNovember 2012 to December 2013\\n\\nEDUCATION\\n\\nB.E in Computer Science Engineering\\n\\nAdithya Institute of Technology -  Tamil Nadu\\n\\nSeptember 2008 to June 2012\\n\\nhttps://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN\\nhttps://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN\\n\\n\\nSKILLS\\n\\nAPEX. (Less than 1 year), Data Structures (3 years), FLEXCUBE (5 years), Oracle (5 years),\\nAlgorithms (3 years)\\n\\nLINKS\\n\\nhttps://www.linkedin.com/in/govardhana-k-61024944/\\n\\nADDITIONAL INFORMATION\\n\\nTechnical Proficiency:\\n\\nLanguages: Core Java, Go Lang, Data Structures & Algorithms, Oracle\\nPL-SQL programming, Sales Force with APEX.\\nTools: RADTool, Jdeveloper, NetBeans, Eclipse, SQL developer,\\nPL/SQL Developer, WinSCP, Putty\\nWeb Technologies: JavaScript, XML, HTML, Webservice\\n\\nOperating Systems: Linux, Windows\\nVersion control system SVN & Git-Hub\\nDatabases: Oracle\\nMiddleware: Web logic, OC4J\\nProduct FLEXCUBE: Oracle FLEXCUBE Versions 10.x, 11.x and 12.x\\n\\nhttps://www.linkedin.com/in/govardhana-k-61024944/\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "oljNrJA3a-E9"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function to print results in a formatted manner\n",
        "def print_output(result):\n",
        "    # Iterate over the result and print each entity and its text\n",
        "    for data in result:\n",
        "        print(f\"{data['entity']} --> {data['text']}\")\n",
        "\n",
        "# Creating a function to get the predicted values\n",
        "def get_results(\n",
        "    text_data,\n",
        "    model=model,\n",
        "    tokenizer=TOKENIZER,\n",
        "    idx2tag=idx2tag,\n",
        "    max_len=MAX_LEN,\n",
        "    device=DEVICE):\n",
        "    # Preprocess the input text data\n",
        "    text_data = preprocess_text(text_data)\n",
        "\n",
        "    # Get the predicted entities using the predict function\n",
        "    entity_res = predict(model, TOKENIZER, idx2tag, DEVICE, text_data, MAX_LEN)\n",
        "\n",
        "    # Print the inputted resume\n",
        "    print(f\"The inputted resume is given below:\\n{textwrap.fill(text_data, 100)}\\n\")\n",
        "\n",
        "    # Print a separator line\n",
        "    print(\"-\"*35)\n",
        "\n",
        "    # Print the output correlated entities\n",
        "    print(f\"The Output correlated entity are as follows:\")\n",
        "    print_output(entity_res)\n"
      ],
      "metadata": {
        "id": "1_qDcUIJg8W-"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_results(text_data = resume_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOaXrQNpifSE",
        "outputId": "05b0859d-d4ca-41ce-b233-e2e04f9e19eb"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The inputted resume is given below:\n",
            "Govardhana K Senior Software Engineer  Bengaluru, Karnataka, Karnataka - Email me on Indeed:\n",
            "indeed.com/r/Govardhana-K/ b2de315d95905b68  Total IT experience 5 Years 6 Months Cloud Lending\n",
            "Solutions INC 4 Month • Salesforce Developer Oracle 5 Years 2 Month • Core Java Developer Languages\n",
            "Core Java, Go Lang Oracle PL-SQL programming, Sales Force Developer with APEX.  Designations &\n",
            "Promotions  Willing to relocate: Anywhere  WORK EXPERIENCE  Senior Software Engineer  Cloud Lending\n",
            "Solutions -  Bangalore, Karnataka -  January 2018 to Present  Present  Senior Consultant  Oracle -\n",
            "Bangalore, Karnataka -  November 2016 to December 2017  Staff Consultant  Oracle -  Bangalore,\n",
            "Karnataka -  January 2014 to October 2016  Associate Consultant  Oracle -  Bangalore, Karnataka -\n",
            "November 2012 to December 2013  EDUCATION  B.E in Computer Science Engineering  Adithya Institute of\n",
            "Technology -  Tamil Nadu  September 2008 to June 2012\n",
            "https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN\n",
            "https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN\n",
            "SKILLS  APEX. (Less than 1 year), Data Structures (3 years), FLEXCUBE (5 years), Oracle (5 years),\n",
            "Algorithms (3 years)  LINKS  https://www.linkedin.com/in/govardhana-k-61024944/  ADDITIONAL\n",
            "INFORMATION  Technical Proficiency:  Languages: Core Java, Go Lang, Data Structures & Algorithms,\n",
            "Oracle PL-SQL programming, Sales Force with APEX. Tools: RADTool, Jdeveloper, NetBeans, Eclipse, SQL\n",
            "developer, PL/SQL Developer, WinSCP, Putty Web Technologies: JavaScript, XML, HTML, Webservice\n",
            "Operating Systems: Linux, Windows Version control system SVN & Git-Hub Databases: Oracle Middleware:\n",
            "Web logic, OC4J Product FLEXCUBE: Oracle FLEXCUBE Versions 10.x, 11.x and 12.x\n",
            "https://www.linkedin.com/in/govardhana-k-61024944/\n",
            "\n",
            "-----------------------------------\n",
            "The Output correlated entity are as follows:\n",
            "Name --> Govardhana K\n",
            "Designation --> Senior Software Engineer\n",
            "Location --> Bengaluru,\n",
            "Location --> ,\n",
            "Companies worked at --> Solutions\n",
            "Designation --> force Developer\n",
            "Companies worked at --> Oracle\n",
            "Companies worked at --> Oracle\n",
            "Designation --> Senior Software Engineer\n",
            "Companies worked at --> Cloud Lending Solutions\n",
            "Designation --> Senior Consultant\n",
            "Companies worked at --> Oracle\n",
            "Location --> Bangalore\n",
            "Designation --> Consultant\n",
            "Companies worked at --> Oracle\n",
            "Designation --> Consultant\n",
            "Companies worked at --> Oracle\n",
            "Degree --> B.E in Computer Science Engineering\n",
            "College Name --> Adithya Institute of Technology\n",
            "Skills --> APEX. (Less than 1 year), Data Structures (3 years), FLEXCUBE (5 years), Oracle (5 years), Algorithms (3 years)\n",
            "Skills --> Java,\n",
            "Skills --> , Data Structures & Algorithms,\n",
            "Skills --> PL-SQL programming,\n",
            "Skills --> Tools: RADTool, Jdeveloper, NetBeans, Eclipse, SQL developer, PL/SQL Developer, WinSCP, Putty Web Technologies: JavaScript, XML, HTML, Webservice\n",
            "Skills --> Operating Systems: Linux, Windows Version control system SVN & Git-Hub Databases\n",
            "Companies worked at --> Oracle\n",
            "Skills --> Middleware: Web logic, OC4J Product FLEXCUBE:\n",
            "Skills --> FLEXCUBE Versions 10.x, 11.x\n",
            "Skills --> 12.x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mubI5wlNowQw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}